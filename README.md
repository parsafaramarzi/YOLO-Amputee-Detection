# ğŸ§˜â€â™‚ï¸ YOLOv11 Pose Integrity & Status Detector

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![YOLOv11](https://img.shields.io/badge/YOLO-v11-magenta)
![OpenCV](https://img.shields.io/badge/OpenCV-Computer%20Vision-green)
![Status](https://img.shields.io/badge/Status-Active-success)

A real-time computer vision application that utilizes **YOLOv11-Pose** to detect human keypoints and analyze physical integrity. The system classifies individuals as **"Healthy"** or **"Disabled"** based on the visibility and confidence of specific anatomical joints.

---

## ğŸ“¸ Visual Demo

Below are actual output frames generated by the system. The overlay displays the skeleton, the classification status ("Healthy" vs "Disabled"), and lists any undetected joints.

<p align="center">
  <img src="output/demo_frame04.jpg" width="45%" alt="Demo Frame 1">
  <img src="output/demo_frame.jpg" width="45%" alt="Demo Frame 2">
</p>
<p align="center">
  <img src="output/demo_frame02.jpg" width="45%" alt="Demo Frame 3">
  <img src="output/demo_frame03.jpg" width="45%" alt="Demo Frame 4">
</p>

---

## ğŸ“ Table of Contents
- [Features](#-features)
- [How It Works](#-how-it-works)
- [Limitations](#-limitations)
- [Project Structure](#-project-structure)
- [Installation](#-installation)
- [Usage](#-usage)
- [Configuration](#-configuration)
- [Output](#-output)

---

## âœ¨ Features

* **Advanced Pose Estimation:** Uses the state-of-the-art `yolov11x-pose` model for high-accuracy keypoint detection.
* **Integrity Analysis:** Automatically detects low-confidence keypoints to identify potential "missing" limbs.
* **Dynamic Visualizations:**
    * Draws full body skeletons.
    * Highlights "Healthy" individuals in **Green** and "Disabled" in **Red**.
    * Lists specific missing body parts (e.g., "Missing: L Wrist").
* **Real-time Dashboard:** Displays a live counter of Healthy vs. Disabled individuals on-screen.
* **Video Export:** Automatically saves the analyzed footage to `.mp4`.

---

## ğŸ§  How It Works

The script processes video frames and utilizes a confidence threshold to determine the status of a person:

1.  **Detection:** The model scans for 12 specific joints: Shoulders, Elbows, Wrists, Hips, Knees, and Ankles.
2.  **Thresholding:** It checks the confidence score (`conf`) of each keypoint.
    * **Threshold:** `0.7` (70%)
3.  **Classification:**
    * **Healthy:** All monitored joints are detected with confidence > 0.7.
    * **Disabled:** One or more joints fall below the threshold. The specific missing joint is logged and displayed above the bounding box.

---

## âš ï¸ Limitations

Please note that this is a proof-of-concept using raw confidence thresholds. **The model currently struggles to distinguish between actual amputation and temporary occlusion.**

* **Occlusion vs. Amputation:** If a healthy person puts their hand in their pocket, hides it behind their back, or stands behind an object, the YOLO model's confidence for that keypoint will drop below 0.7.
* **False Positives:** Consequently, healthy individuals are often misclassified as "Disabled" during complex movements or when limbs are obscured from the camera's view.

Future improvements could involve temporal tracking (remembering a limb existed in previous frames) to reduce these false positives.

---

## ğŸ“‚ Project Structure

The script utilizes the `output` folder to store both the video and the demo frames shown above.

```text
ğŸ“¦ YOLO-Amputee-Detection
 â”£ ğŸ“‚ output
 â”ƒ â”£ ğŸ“œ demo_frame.jpg        # Generated snapshots used in README
 â”ƒ â”£ ğŸ“œ demo_frame01.jpg
 â”ƒ â”£ ğŸ“œ demo_frame02.jpg
 â”ƒ â”£ ğŸ“œ demo_frame03.jpg
 â”ƒ â”— ğŸ“œ demo_frame04.jpg
 â”— ğŸ“œ main.py                 # The analysis script
```

---

## âš™ï¸ Installation

1.  **Clone the repository**
    ```bash
    git clone https://github.com/yourusername/yolo-pose-integrity.git
    cd yolo-pose-integrity
    ```

2.  **Install Dependencies**
    It is recommended to use a virtual environment.
    ```bash
    pip install ultralytics opencv-python imageio
    ```

3.  **Download Weights**
    Ensure `yolo11x-pose.pt` is in the root directory. If not, the `ultralytics` package will attempt to download it automatically on the first run.

---

## ğŸš€ Usage

1.  Place your input video in the `dataset/` folder (rename it to `05.mp4` or update the code).
2.  Run the script:
    ```bash
    python main.py
    ```
3.  **Controls:**
    * The processing window will open.
    * Press **`Enter`** to stop processing early and save the video.

---

## ğŸ›  Configuration

You can toggle specific features by modifying the boolean flags at the top of the script:

| Variable | Default | Function |
| :--- | :--- | :--- |
| `draw_pose_flag` | `True` | Draws the skeleton lines and joint dots. |
| `draw_boxes_flag` | `True` | Draws the bounding box and status text. |
| `display_status_counts_flag` | `True` | Shows the black dashboard with counts in top-left. |

---

## ğŸ“¹ Output

The project exports a processed video file using `imageio` with `libx264` encoding.

* **Location:** `output/yolov11_pose_estimator_detected.mp4`
* **Quality:** Level 8
* **FPS:** 30

---

### ğŸ¤ Contributing

Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.
